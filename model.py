import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric.nn as pygnn
from torch_geometric.nn import (
    GCNConv, SAGEConv, GATConv, ResGatedGraphConv, 
    GINEConv, ClusterGCNConv
)
from torch_geometric.nn.models.mlp import MLP
from torch_geometric.nn.aggr import AttentionalAggregation
from gps_layer import GPSLayer


# NET = 0
# DEV = 1
# PIN = 2


class GraphHead(nn.Module):
    """ GNN head for graph-level prediction.

    Implementation adapted from the transductive GraphGPS.

    Args:
        hidden_dim (int): Hidden features' dimension
        dim_out (int): Output dimension. For binary prediction, dim_out=1.
        num_layers (int): Number of layers of GNN model
        layers_post_mp (int): number of layers of head MLP
        use_bn (bool): whether to use batch normalization
        drop_out (float): dropout rate
        activation (str): activation function
        src_dst_agg (str): the way to aggregate src and dst nodes, which can be 'concat' or 'add' or 'pool'
    """
    def __init__(self, args):
        super().__init__()
        self.use_cl = False
        self.use_stats = args.use_stats
        hidden_dim = args.hid_dim
        node_embed_dim = hidden_dim
        self.task = args.task
        self.task_level = args.task_level
        self.net_only = args.net_only
        self.num_classes = args.num_classes
        self.class_boundaries = args.class_boundaries
        # æ·»åŠ è¿™ä¸€è¡Œ
        self.dataset_name = args.dataset
        local_gnn_type = args.local_gnn_type
        global_model_type = args.global_model_type
        act_fn = args.act_fn
        attn_dropout = args.attn_dropout
        use_bn = args.use_bn
        num_heads = args.num_heads
        dropout = args.dropout
        residual = args.residual
        g_bn = args.g_bn
        g_drop = args.g_drop
        g_ffn = args.g_ffn
        layer_norm = args.layer_norm
        batch_norm = args.batch_norm
        task_level = args.task_level
        
        ## circuit statistics encoder + PE encoder + node&edge type encoders
        #ç¡®ä¿éšè—ç»´åº¦å±‚æ•°è¢«ä¸‰æ•´é™¤å¹¶å°†å®ƒå¹³å‡åˆ†é…ç»™ä¸‰ç§ç¼–ç å™¨
        # if args.use_stats + self.use_cl == 2:
        #     assert hidden_dim % 3 == 0, \
        #         "hidden_dim should be divided by 3 (3 types of encoders)"
        #     node_embed_dim = hidden_dim // 3

        # ## circuit statistics/pe encoder + node&edge type encoders
        # #åŒç†
        # elif self.use_stats + self.use_cl == 1:
        #     assert hidden_dim % 2 == 0, \
        #         "hidden_dim should be divided by 2 (2 types of encoders)"
        #     node_embed_dim = hidden_dim // 2

        # ## only use node&edge type encoders
        # else:
        #     pass

        # ## Contrastive learning encoder
        # #å›¾å¯¹æ¯”å­¦ä¹ ç”¨åˆ°çš„ç¼–ç å™¨
        # if self.use_cl:
        #     self.cl_linear = nn.Linear(args.cl_hid_dim, node_embed_dim)

        ## Circuit Statistics encoder, producing matrix C
        # if self.use_stats:#ä¸ºä¸åŒç±»å‹èŠ‚ç‚¹å®šä¹‰ä¸åŒçš„ç¼–ç å™¨ï¼Œè¾“å…¥ç»´åº¦17ï¼Œè¾“å‡ºç»´åº¦node_embed_dim
        #     ## add node_attr transform layer for net/device/pin nodes, by shan
        #     self.net_attr_layers = nn.Linear(17, node_embed_dim, bias=True)
        #     self.dev_attr_layers = nn.Linear(17, node_embed_dim, bias=True)
        #     ## pin attributes are {0, 1, 2} for gate pin, source/drain pin, and base pin
        #     self.pin_attr_layers = nn.Embedding(17, node_embed_dim)
        #     self.c_embed_dim = node_embed_dim
        
        #å®šä¹‰èŠ‚ç‚¹å’Œè¾¹çš„ç¼–ç å™¨ï¼Œå°†èŠ‚ç‚¹ç±»å‹å’Œè¾¹ç±»å‹ç¼–ç ä¸ºå‘é‡
        ## Node / Edge type encoders.
        ## Node attributes are {0, 1, 2} for net, device, and pin
        # åœ¨ __init__ æ–¹æ³•ä¸­ï¼Œnode_encoder å®šä¹‰ä¹‹åæ·»åŠ 
        if self.task_level == 'node':
            node_type_vocab_size = getattr(args, 'node_type_vocab_size', 142)
            self.node_encoder = nn.Embedding(num_embeddings=node_type_vocab_size, embedding_dim=node_embed_dim)
            
            # æ·»åŠ ç¬¬å››åˆ—å’Œç¬¬å…­åˆ—çš„ embedding å±‚
            col4_vocab_size = getattr(args, 'col4_vocab_size', 8)  # æ ¹æ®å®é™…æ•°æ®è°ƒæ•´
            col6_vocab_size = getattr(args, 'col6_vocab_size', 2)  # æ ¹æ®å®é™…æ•°æ®è°ƒæ•´
            self.col4_encoder = nn.Embedding(num_embeddings=col4_vocab_size, embedding_dim=node_embed_dim)
            self.col6_encoder = nn.Embedding(num_embeddings=col6_vocab_size, embedding_dim=node_embed_dim)
            self.edge_encoder = nn.Embedding(num_embeddings=4, embedding_dim=node_embed_dim)       
            concatenated_dim = 3 * node_embed_dim + 11
            self.feature_projection = nn.Linear(concatenated_dim, node_embed_dim)
            
        # GNN layers
        self.layers = nn.ModuleList()
        self.model = args.model
        #é€‰æ‹©MPNNæ¨¡å‹ï¼Œè¯¦è§aspdac
        for _ in range(args.num_gnn_layers): #é€‰æ‹©ä¸€èˆ¬è®­ç»ƒæ¨¡å‹
            ## the following are examples of using different GNN layers
            if args.model == 'clustergcn':
                self.layers.append(ClusterGCNConv(hidden_dim, hidden_dim))
            elif args.model == 'gcn':
                self.layers.append(GCNConv(hidden_dim, hidden_dim))
            elif args.model == 'sage':
                self.layers.append(SAGEConv(hidden_dim, hidden_dim))
            elif args.model == 'gat':
                self.layers.append(GATConv(hidden_dim, hidden_dim, heads=1))
            elif args.model == 'resgatedgcn':
                self.layers.append(ResGatedGraphConv(hidden_dim, hidden_dim, edge_dim=hidden_dim))
            elif args.model == 'gine':
                mlp = MLP(
                    in_channels=hidden_dim, 
                    hidden_channels=hidden_dim, 
                    out_channels=hidden_dim, 
                    num_layers=2, 
                    norm=None,
                )
                self.layers.append(GINEConv(mlp, train_eps=True, edge_dim=hidden_dim))
            # elif args.model == 'CustomGatedGCN':
            #     self.layers.append(GatedGCNLayer(in_dim=hidden_dim, 
            #                                     out_dim=hidden_dim,
            #                                 dropout=g_drop,
            #                                 residual=residual,
            #                                 ffn=g_ffn,
            #                                 batch_norm=g_bn,
            #                                 ))
            # elif args.model == 'CustomGCNConv':
            #     self.layers.append(GCNConvLayer(dim_in=hidden_dim, 
            #                                     dim_out=hidden_dim,
            #                                 dropout=g_drop,
            #                                 residual=residual,
            #                                 ffn=g_ffn,
            #                                 batch_norm=g_bn,
            #                                 ))
            # elif args.model == 'CustomGINEConv':
            #     self.layers.append(GINEConvLayer(dim_in=hidden_dim, 
            #                                     dim_out=hidden_dim,
            #                                 dropout=g_drop,
            #                                 residual=residual,
            #                                 ffn=g_ffn,
            #                                 batch_norm=g_bn))
            elif args.model == 'gps_attention': #å¯ç”¨ä¸Šæ¸¸å›¾æ³¨æ„åŠ›æœºåˆ¶
                self.layers.append(GPSLayer(hid_dim=hidden_dim, 
                                            local_gnn_type=local_gnn_type,
                                            global_model_type=global_model_type, 
                                            act=act_fn, 
                                            attn_dropout=attn_dropout, 
                                            batch_norm=batch_norm,
                                            layer_norm=layer_norm,
                                            num_heads=num_heads,
                                            residual=residual,
                                            g_bn=g_bn,
                                            g_drop=g_drop,
                                            g_ffn=g_ffn,
                                            task_level=task_level
                                            ))
            else:
                raise ValueError(f'Unsupported GNN model: {args.model}')
        
        self.src_dst_agg = args.src_dst_agg #æŒ‡å®šèšåˆæ–¹å¼

        ## Add graph pooling layer
        if args.src_dst_agg == 'pooladd': 
            self.pooling_fun = pygnn.pool.global_add_pool #å®šä¹‰èšåˆä¸ºåŠ å’Œ
            
        elif args.src_dst_agg == 'poolmean':
            self.pooling_fun = pygnn.pool.global_mean_pool #å®šä¹‰èšåˆä¸ºå¹³å‡å€¼
            
        elif args.src_dst_agg == 'globalattn': #å®šä¹‰å…¨å±€æ³¨æ„åŠ›æœºåˆ¶
            self.attn_nn = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
            )
            self.pooling_fun = AttentionalAggregation(gate_nn=self.attn_nn)
        ## The head configuration
        head_input_dim = hidden_dim * 2 if self.src_dst_agg == 'concat'  and self.task_level == 'edge' else hidden_dim #å¦‚æœåµŒå…¥è¾¹ç‰¹å¾ï¼Œåˆ™è¾“å…¥ç»´åº¦ä¸ºhidden_dim*2ï¼Œå¦åˆ™ä¸ºhidden_dim
        
        #å®šä¹‰æ¨¡å‹è¾“å‡ºç»´åº¦
        if self.task == 'regression':
            dim_out = 1
        elif self.task =='classification':
            dim_out = args.num_classes
        else:
            raise ValueError('Invalid task')
        
        # head MLP layers å®šä¹‰MLPï¼Œæ¥æ”¶GNN+çš„è¾“å‡ºï¼Œè¾“å‡ºç»´åº¦ä¸º1æˆ–num_classesï¼Œè¯¦è§aspdac
        self.head_layers = MLP(
            in_channels=head_input_dim, 
            hidden_channels=hidden_dim, 
            out_channels=dim_out, 
            num_layers=args.num_head_layers, 
            use_bn=use_bn, dropout=dropout, 
            activation=args.act_fn,
        )
        
        ## Batch normalization #å±‚å½’ä¸€åŒ–
        self.use_bn = args.use_bn
        self.bn_node_x = nn.BatchNorm1d(hidden_dim)
        if self.use_bn and self.use_cl:
            print("[Warning] Using batch normalization with contrastive learning may cause performance degradation.")

        ## activation setting #å®šä¹‰æ¿€æ´»å‡½æ•°
        if args.act_fn == 'relu':
            self.activation = nn.ReLU()
        elif args.act_fn == 'elu':
            self.activation = nn.ELU()
        elif args.act_fn == 'tanh':
            self.activation = nn.Tanh()
        elif args.act_fn == 'leakyrelu':
            self.activation = nn.LeakyReLU()
        elif args.act_fn == 'prelu':
            self.activation = nn.PReLU()
        else:
            raise ValueError('Invalid activation')
        
        ## Dropout setting æ­£åˆ™åŒ–
        self.drop_out = args.dropout
        
        if self.task_level == 'node' and args.src_dst_agg == 'globalattn':
            print("[Warning] global attention is not typically used for node-level tasks.")

        #å‰å‘ä¼ æ’­
    def forward(self, batch):
        if self.task_level == 'node':
            if self.dataset_name == 'integrated_position_prediction_graph':
                # æå–éœ€è¦embeddingçš„ç¦»æ•£ç‰¹å¾åˆ—ï¼ˆæ³¨æ„ï¼šPythonç´¢å¼•ä»0å¼€å§‹ï¼‰
                node_type_ids = batch.x[:, 2].long()  # ç¬¬3ç»´ï¼šèŠ‚ç‚¹ç±»å‹
                col4_ids = batch.x[:, 3].long()       # ç¬¬4ç»´ï¼šéœ€è¦embeddingçš„ç‰¹å¾
                col6_ids = batch.x[:, 5].long()       # ç¬¬6ç»´ï¼šéœ€è¦embeddingçš„ç‰¹å¾
                edge_type_ids = batch.edge_attr[batch.e_id, 1].long()
                
                # å¯¹å„ä¸ªç¦»æ•£ç‰¹å¾è¿›è¡Œé‡æ˜ å°„
                unique_node_types, node_inverse = torch.unique(node_type_ids, return_inverse=True)
                unique_col4_types, col4_inverse = torch.unique(col4_ids, return_inverse=True)
                unique_col6_types, col6_inverse = torch.unique(col6_ids, return_inverse=True)
                unique_edge_types, edge_inverse = torch.unique(edge_type_ids, return_inverse=True)
                
                # # ğŸ” æ‰“å°é‡æ˜ å°„ä¿¡æ¯
                # print(f"ğŸ” é‡æ˜ å°„ä¿¡æ¯:")
                # print(f"  unique_node_typesæ•°é‡: {len(unique_node_types)}, æœ€å¤§é‡æ˜ å°„ID: {node_inverse.max()}")
                # print(f"  unique_col4_typesæ•°é‡: {len(unique_col4_types)}, æœ€å¤§é‡æ˜ å°„ID: {col4_inverse.max()}")
                # print(f"  unique_col6_typesæ•°é‡: {len(unique_col6_types)}, æœ€å¤§é‡æ˜ å°„ID: {col6_inverse.max()}")
                # print(f"  unique_edge_typesæ•°é‡: {len(unique_edge_types)}, æœ€å¤§é‡æ˜ å°„ID: {edge_inverse.max()}")
                
                # åˆ›å»ºé‡æ˜ å°„çš„ID
                remapped_node_type_ids = node_inverse.to(batch.x.device)
                remapped_col4_ids = col4_inverse.to(batch.x.device)
                remapped_col6_ids = col6_inverse.to(batch.x.device)
                remapped_edge_type_ids = edge_inverse.to(batch.x.device)
                
                # é€šè¿‡embeddingå±‚ç¼–ç ç¦»æ•£ç‰¹å¾
                x_node = self.node_encoder(remapped_node_type_ids)  # embeddingç»´åº¦
                x_col4 = self.col4_encoder(remapped_col4_ids)       # embeddingç»´åº¦
                x_col6 = self.col6_encoder(remapped_col6_ids)       # embeddingç»´åº¦
                xe = self.edge_encoder(remapped_edge_type_ids)

                # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾ï¼šè¿ç»­ç‰¹å¾ + ä¸‰ä¸ªembeddingç‰¹å¾
                continuous_features = torch.cat([
                    batch.x[:, :2],      # ç¬¬0ã€1ç»´
                    batch.x[:, 4:5],     # ç¬¬4ç»´ï¼ˆç´¢å¼•4ï¼Œå®é™…ç¬¬5ç»´ï¼‰
                    batch.x[:, 6:]       # ç¬¬6ç»´ä¹‹åçš„æ‰€æœ‰ç»´åº¦
                ], dim=1)
                
                x = torch.cat([continuous_features, x_node, x_col4, x_col6], dim=1)
                # print(f"  batch.xå½¢çŠ¶: {x.shape}")
                x = self.feature_projection(x)
        
            # elif self.args.dataset == 'integrated_power_density_prediction_graph':
        # GNN layers
            # print(f"  batch.xå½¢çŠ¶: {x.shape}")
            # print(f"  batch.edge_attrå½¢çŠ¶: {xe.shape}")
        if self.model == 'gps_attention':
            batch.x = x
            batch.edge_attr = xe
            # print(f"ğŸ” è¿›å…¥GPSå±‚å‰:")
            # print(f"  batch.xå½¢çŠ¶: {batch.x.shape}")
            # print(f"  batch.edge_attrå½¢çŠ¶: {batch.edge_attr.shape}")
            # print(f"  batch.edge_indexå½¢çŠ¶: {batch.edge_index.shape}")
            # print(f"  è¾¹æ•°é‡: {batch.edge_index.shape[1]}, è¾¹å±æ€§æ•°é‡: {batch.edge_attr.shape[0]}")
    
            for i, layer in enumerate(self.layers):
                # print(f"ğŸ” GPSå±‚ {i+1} å¤„ç†å‰ batch.xå½¢çŠ¶: {batch.x.shape}")
                batch = layer(batch)
                # print(f"ğŸ” GPSå±‚ {i+1} å¤„ç†å batch.xå½¢çŠ¶: {batch.x.shape}")
            x = batch.x
            # print(f"ğŸ” æ‰€æœ‰GPSå±‚å¤„ç†å®Œæˆåæœ€ç»ˆxå½¢çŠ¶: {x.shape}")
            # assert 0
        else:
            # x = x.float()
            #ä¸ç”¨æ³¨æ„åŠ›ä½¿ç”¨æ™®é€šçš„GNN
            for conv in self.layers:
                if self.model == 'gine' or self.model == 'resgatedgcn' :
                    x = conv(x, batch.edge_index, edge_attr=xe)
                elif self.model == 'CustomGatedGCN' or self.model == 'CustomGCNConv' or self.model == 'CustomGINEConv':
                    batch.x = x
                    batch.edge_attr = xe
                    batch = conv(batch)
                    x = batch.x
                else:
                    x = conv(x, batch.edge_index)
                #å½’ä¸€åŒ–ï¼Œæ¿€æ´»ä¸æ­£åˆ™åŒ–
                if self.use_bn:
                    x = self.bn_node_x(x)
                x = self.activation(x)

                if self.drop_out > 0.0:
                    x = F.dropout(x, p=self.drop_out, training=self.training)

        #ä¸‹æ¸¸æ¨¡å‹ï¼Œåˆ©ç”¨MLPåˆ†ç±»æˆ–é¢„æµ‹
        ## task level : node
        if self.task_level == 'node':
            # if self.net_only:
            #     net_node_mask = batch.node_type == NET
            #     pred = self.head_layers(x[net_node_mask])
            #     true_class = batch.y[:, 1][net_node_mask].long()
            #     true_label = batch.y[net_node_mask]
            # else:
            print(f"x.size:{x.size()}")
            print(f"x:{x}")
            
            pred = self.head_layers(x)
            # åˆ é™¤ä»¥ä¸‹4è¡Œè°ƒè¯•æ‰“å°è¯­å¥
            print(f"ğŸ” ä¸‹æ¸¸æ¨¡å‹è¾“å‡º pred å½¢çŠ¶: {pred.shape}")
            print(f"ğŸ” ä¸‹æ¸¸æ¨¡å‹è¾“å‡º pred å†…å®¹: {pred}")
            # print(f"batch.y:{batch.y}")
            # print(f"batch.y.size:{batch.y.size()}")
            assert 0
            true_class = batch.y.long()
            true_label = batch.y

        elif self.task_level == 'edge': 
            pred = self.head_layers(x)
            true_class = batch.y.long()
            true_label = batch.y
        # elif self.task_level == 'edge':
        #     if self.src_dst_agg[:4] == 'pool': #è®­ç»ƒå‰å…ˆæ± åŒ–ï¼Œä¸€èˆ¬å°±æ˜¯concatè€Œä¸æ˜¯pool
        #         graph_emb = self.pooling_fun(x, batch.batch)
        #     else:
        #         batch_size = batch.edge_label.size(0)
        #         src_emb = x[:batch_size, :]
        #         dst_emb = x[batch_size:batch_size*2, :]
        #         if self.src_dst_agg == 'concat':
        #             graph_emb = torch.cat((src_emb, dst_emb), dim=1)
        #         else:
        #             graph_emb = src_emb + dst_emb

        #     #è·å¾—é¢„æµ‹è¾“å‡ºï¼Œå¹¶è¿”å›çœŸå®å€¼ä¾›åç»­è®¡ç®—æŸå¤±
        #     pred = self.head_layers(graph_emb)
        #     true_class = batch.edge_label[:, 1].long()
        #     true_label = batch.edge_label
        
        else:
            raise ValueError('Invalid task level')
        
        # print(f"pred:{pred},true_class:{true_class},true_label:{true_label}")
        # print(f"pred:{pred},true_label:{true_label}")
        # pred_squeezed = pred.squeeze()

        # # è®¡ç®—ç»å¯¹è¯¯å·®
        # absolute_error = torch.abs(pred_squeezed - true_label)
        # print(f"ç»å¯¹è¯¯å·®å½¢çŠ¶: {absolute_error.shape}")  # [N]
        # print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {torch.mean(absolute_error).item():.4f}")       
        # print(f"pred_squeezed èŒƒå›´: {pred_squeezed.min():.4f} ~ {pred_squeezed.max():.4f}")
        # print(f"true_label èŒƒå›´: {true_label.min():.4f} ~ {true_label.max():.4f}") 
        # assert 0
        return pred,true_class,true_label
    

