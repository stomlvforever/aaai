
import torch
import uvicorn
import json
import os
import sys
import asyncio
import uuid
import numpy as np
import logging
import argparse
import builtins
from pathlib import Path
from torch.utils.data import Subset
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import Optional, Dict, List, Any, Union
from datetime import datetime
from torch.utils.data import Subset
from torch_geometric.loader import NeighborLoader, LinkNeighborLoader
# å¯¼å…¥ä½ çš„8æ®µä»£ç æ¨¡å—
# å‡è®¾è¿™äº›æ–‡ä»¶åœ¨å½“å‰ç›®å½•æˆ–Pythonè·¯å¾„ä¸­
try:
    # å¯¼å…¥ä½ çš„æ¨¡å— - å¯¹åº”8æ®µä»£ç 
    from downstream_train import downstream_train, Logger, eval_epoch, regress_train, class_train, compute_loss
    from gps_layer import GPSLayer
    from layer import GatedGCNLayer, GCNConvLayer, GINEConvLayer
    from model import GraphHead
    from plot import visualize_node_label_distribution, visualize_edge_label_distribution
    from sampling import dataset_sampling
    from sram_dataset import performat_SramDataset, SealSramDataset  
    """
    å°è¯•å¯¼å…¥é¡¹ç›®çš„æ ¸å¿ƒæ¨¡å—ï¼ˆ8æ®µä»£ç å¯¹åº”çš„åŠŸèƒ½ï¼‰
    å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè®¾ç½®MODULES_AVAILABLE = Falseï¼ŒAPIä¼šè¿›å…¥æ¨¡æ‹Ÿæ¨¡å¼
    è¿™ç§è®¾è®¡ä¿è¯äº†APIå³ä½¿åœ¨ç¼ºå°‘æŸäº›ä¾èµ–æ—¶ä¹Ÿèƒ½è¿è¡Œï¼ˆç”¨äºæµ‹è¯•ï¼‰
    """
    # æ¨¡æ‹ŸRCG graphæ¨¡å—
    class PyGraphDataset:
        """æ¨¡æ‹Ÿ rcg.graph.PyGraphDataset"""
        def __init__(self, name, task):
            self.name = name
            self.task = task
            if task in ["nodeclass", "noderegress"]:
                task_level = "node"
            else:
                task_level = "edge"            
            # ä½¿ç”¨ä½ çš„SealSramDataset
            self.dataset = SealSramDataset(
                name=name,
                root="./datasets/",
                task_level=task_level,
                class_boundaries=[0.2, 0.4, 0.6, 0.8]
            )
            if len(self.dataset) > 0:
                first_graph = self.dataset[0]
                print(f"DEBUG_API: In PyGraphDataset __init__, after SealSramDataset, first_graph has tar_edge_dist? {hasattr(first_graph, 'tar_edge_dist')}")
            else:
                print("DEBUG_API: Dataset is empty after initialization.")  
                      
        def get_idx_split(self):
            """è·å–æ•°æ®åˆ†å‰²ç´¢å¼•
            æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©èŠ‚ç‚¹æ•°é‡æˆ–è¾¹æ•°é‡ä½œä¸ºåˆ†å‰²åŸºç¡€
            æŒ‰6:2:2æ¯”ä¾‹åˆ†å‰²è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
            ä½¿ç”¨éšæœºæ’åˆ—ç¡®ä¿æ•°æ®åˆ†å¸ƒçš„éšæœºæ€§
            """
            if len(self.dataset) == 0:
                num_nodes = 1000
            else:
                num_nodes = self.dataset[0].num_nodes if self.task in ["nodeclass", "noderegress"] else self.dataset[0].num_edges
            
            indices = np.random.permutation(num_nodes)
            
            train_size = int(0.6 * num_nodes)
            val_size = int(0.2 * num_nodes)
            
            return {
                "train": indices[:train_size],
                "valid": indices[train_size:train_size+val_size], 
                "test": indices[train_size+val_size:]
            }
        
        def get_dataloader(self, split_indices):
            """
            åˆ†ä¸¤ç§è°ƒç”¨æ–¹å¼ï¼š
            1) dataset.get_dataloader(train_idx: Tensor/ndarray/list)
            â†’ åªä¸ºè¿™ç»„èŠ‚ç‚¹/è¾¹ç”Ÿæˆå•ä¸€çš„ Loader
            2) dataset.get_dataloader({
                'train': train_idx,
                'valid': valid_idx,
                'test': test_idx
            })
            â†’ è¿”å› (train_loader, valid_loader, test_loaders_dict)
            """
            # é€šç”¨å‚æ•°æ„é€ 
            if self.task in ["nodeclass", "noderegress"]:
                task_level = "node"; task_type = \
                    "classification" if self.task=="nodeclass" else "regression"
            else:
                task_level = "edge"; task_type = \
                    "classification" if self.task=="edgeclass" else "regression"

            args = argparse.Namespace(
                task_level=task_level,
                task=task_type,
                num_hops=4,
                num_neighbors=64,
                batch_size=128,
                num_workers=0,
                net_only=0,
                small_dataset_sample_rates=1.0,
                large_dataset_sample_rates=0.01,
            )

            # â€”â€” æƒ…å†µ Aï¼šä¸€æ¬¡ä¼ ä¸€ä¸ª splitï¼ˆTensor/list/ndarrayï¼‰ â€”â€” 
            if not isinstance(split_indices, dict):
                # è½¬æˆ LongTensor
                if isinstance(split_indices, torch.Tensor):
                    idx = split_indices
                elif hasattr(split_indices, "tolist"):
                    idx = torch.tensor(split_indices, dtype=torch.long)
                else:
                    idx = torch.tensor(list(split_indices), dtype=torch.long)

                g = self.dataset[0]  # åªåœ¨ç¬¬ä¸€ä¸ªå›¾ä¸Šé‡‡æ ·

                if task_level == "node":
                    return NeighborLoader(
                        g,
                        num_neighbors=args.num_hops * [args.num_neighbors],
                        input_nodes=idx,
                        batch_size=args.batch_size,
                        shuffle=(task_type=="classification"),
                        num_workers=args.num_workers,
                    )
                else:
                    # edge ä»»åŠ¡è¦ç”¨ LinkNeighborLoader
                    print(f"g:{g}")
                    edge_idx   = g.edge_label_index[:, idx]
                    edge_label = g.edge_label[idx]
                    return LinkNeighborLoader(
                        g,
                        num_neighbors=args.num_hops * [args.num_neighbors],
                        edge_label_index=edge_idx,
                        edge_label=edge_label,
                        subgraph_type="bidirectional",
                        disjoint=True,
                        batch_size=args.batch_size,
                        shuffle=(task_type=="classification"),
                        num_workers=0,
                    )

            # â€”â€” æƒ…å†µ Bï¼šä¸€æ¬¡ä¼ ä¸‰è·¯ splitï¼ˆdictï¼‰ â€”â€” 
            # è€é€»è¾‘ä¸å˜ï¼Œdataset_sampling ä¼šè¿”å› (train_loader, val_loader, test_loaders, max_label)
            train_idx = split_indices["train"]
            val_idx   = split_indices["valid"]
            loaders   = dataset_sampling(
                args,
                self.dataset,
                train_idx=train_idx,
                val_idx=val_idx
            )
            # loaders == (train_loader, val_loader, test_loaders, max_label)
            return loaders[:3]

    
    class Evaluator:
        """æ¨¡æ‹Ÿ rcg.graph.Evaluator
        æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡
        åˆ†ç±»ä»»åŠ¡ï¼šå‡†ç¡®ç‡(accuracy)ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡(precision)ã€å¬å›ç‡(recall)
        å›å½’ä»»åŠ¡ï¼šå‡æ–¹è¯¯å·®(MSE)ã€å¹³å‡ç»å¯¹è¯¯å·®(MAE)ã€RÂ²å†³å®šç³»æ•°
        ä½¿ç”¨average='macro'è®¡ç®—å¤šç±»åˆ†ç±»çš„å¹³å‡æŒ‡æ ‡
        """
        def __init__(self, name, task):
            self.name = name
            self.task = task
        
        def eval(self, input_dict):
            """è¯„ä¼°å‡½æ•°"""
            from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
            y_true = input_dict["y_true"]
            y_pred = input_dict["y_pred"]
            
            if self.task in ["nodeclass", "edgeclass"]:
                # åˆ†ç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡
                return {
                    "accuracy": float(accuracy_score(y_true, y_pred)),
                    "f1": float(f1_score(y_true, y_pred, average='macro')),
                    "precision": float(precision_score(y_true, y_pred, average='macro')),
                    "recall": float(recall_score(y_true, y_pred, average='macro'))
                }
            else:
                # å›å½’ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡
                return {
                    "mse": float(mean_squared_error(y_true, y_pred)),
                    "mae": float(mean_absolute_error(y_true, y_pred)),
                    "r2": float(r2_score(y_true, y_pred))
                }
    
    MODULES_AVAILABLE = True
    
except ImportError as e:
    print(f"Warning: Could not import modules: {e}")
    print("Running in simulation mode...")
    MODULES_AVAILABLE = False
    
    # åˆ›å»ºæ¨¡æ‹Ÿç±»
    class PyGraphDataset:
        def __init__(self, name, task):
            self.name = name
            self.task = task
        
        def get_idx_split(self):
            num_nodes = np.random.randint(1000, 5000)
            indices = np.random.permutation(num_nodes)
            train_size = int(0.6 * num_nodes)
            val_size = int(0.2 * num_nodes)
            return {
                "train": indices[:train_size],
                "valid": indices[train_size:train_size+val_size],
                "test": indices[train_size+val_size:]
            }
        
        def get_dataloader(self, split_indices):
            return None
    
    class Evaluator:
        def __init__(self, name, task):
            self.name = name
            self.task = task
        
        def eval(self, input_dict):
            if self.task == "nodeclass":
                return {
                    "accuracy": np.random.uniform(0.7, 0.95),
                    "f1": np.random.uniform(0.65, 0.9),
                    "precision": np.random.uniform(0.7, 0.9),
                    "recall": np.random.uniform(0.65, 0.85)
                }
            else:
                return {
                    "mse": np.random.uniform(0.1, 0.5),
                    "mae": np.random.uniform(0.05, 0.3),
                    "r2": np.random.uniform(0.7, 0.95)
                }

"""
åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹ï¼Œè®¾ç½®APIå…ƒä¿¡æ¯
å…¨å±€å­—å…¸ç”¨äºå­˜å‚¨è®­ç»ƒä»»åŠ¡çŠ¶æ€ã€è¯„ä¼°ç»“æœã€æ•°æ®é›†ç¼“å­˜
å®šä¹‰æ”¯æŒçš„æ•°æ®é›†å’Œä»»åŠ¡ç±»å‹
"""
app = FastAPI(
    title="RCG Circuit Graph Analysis API",
    version="1.0.0", 
    description="API for RCG project - Circuit Graph Neural Network Training and Evaluation"
)

# å…¨å±€å˜é‡
training_tasks = {}
evaluation_results = {}
dataset_cache = {}

# å¯ç”¨æ•°æ®é›†
AVAILABLE_DATASETS = [
    "sandwich", "ultra8t", "ssram", "sp8192w",
    "digtime", "timing_ctrl", "array_128_32_8t"
]

# å¯ç”¨ä»»åŠ¡
# AVAILABLE_TASKS = ["nodeclass", "regression", "classification"]
AVAILABLE_TASKS = ["nodeclass", "noderegress", "edgeclass", "edgeregress"]
# ==================== æ•°æ®æ¨¡å‹ ====================

class DatasetRequest(BaseModel):
    """æ•°æ®é›†è¯·æ±‚ - å¯¹åº” PyGraphDataset(name="ssram", task="nodeclass")"""
    name: str  # æ•°æ®é›†åç§°
    task: str = "nodeclass"  # ä»»åŠ¡ç±»å‹

class TrainingRequest(BaseModel):
    """è®­ç»ƒè¯·æ±‚ - åŸºäºä½ çš„8æ®µä»£ç """
    # æ•°æ®é›†é…ç½®
    dataset_name: str
    task: str = "nodeclass"
    
    # è®­ç»ƒå‚æ•° - å¯¹åº”ä½ çš„argparseå‚æ•°
    task_level: str = "node"  # "node" or "edge"
    epochs: int = 200
    batch_size: int = 128
    lr: float = 0.0001
    
    # æ¨¡å‹å‚æ•°
    model: str = "gps_attention"  # å¯¹åº”ä½ çš„GraphHeadæ¨¡å‹é€‰æ‹©
    num_gnn_layers: int = 4
    num_head_layers: int = 2
    hid_dim: int = 144
    dropout: float = 0.3
    act_fn: str = "prelu"
    
    # å…¨å±€æ³¨æ„åŠ›è®¾ç½® - å¯¹åº”ä½ çš„GPSLayer
    global_model_type: str = "None"
    local_gnn_type: str = "CustomGatedGCN"
    num_heads: int = 2
    attn_dropout: float = 0.7
    
    # è®¾å¤‡è®¾ç½®
    gpu: int = 0
    seed: int = 42
    
    # å…¶ä»–è®¾ç½®
    use_stats: int = 1
    net_only: int = 0
    neg_edge_ratio: float = 0.5

class EvaluationRequest(BaseModel):
    """è¯„ä¼°è¯·æ±‚ - å¯¹åº” Evaluator.eval(input_dict)"""
    dataset_name: str
    task: str = "nodeclass"
    y_true: List[Union[int, float]]  # çœŸå®æ ‡ç­¾
    y_pred: List[Union[int, float]]  # é¢„æµ‹æ ‡ç­¾

class TaskStatus(BaseModel):
    """ä»»åŠ¡çŠ¶æ€"""
    task_id: str
    status: str
    progress: float
    message: str
    start_time: datetime
    end_time: Optional[datetime] = None
    results: Optional[Dict] = None

class VisualizationRequest(BaseModel):
    """å¯è§†åŒ–è¯·æ±‚ - å¯¹åº”ä½ çš„plotæ¨¡å—"""
    dataset_name: str
    task_level: str = "node"  # "node" or "edge"
    class_boundaries: List[float] = [0.2, 0.4, 0.6, 0.8]

# ==================== æ ¸å¿ƒå‡½æ•° ====================
"""
è¾“å…¥éªŒè¯å‡½æ•°ï¼Œç¡®ä¿ç”¨æˆ·æä¾›çš„æ•°æ®é›†åç§°å’Œä»»åŠ¡ç±»å‹æœ‰æ•ˆ
æŠ›å‡ºHTTP 400é”™è¯¯ï¼Œæä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
"""
def validate_dataset(name: str):
    """éªŒè¯æ•°æ®é›†åç§°"""
    if name not in AVAILABLE_DATASETS:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid dataset '{name}'. Available: {AVAILABLE_DATASETS}"
        )

def validate_task(task: str):
    """éªŒè¯ä»»åŠ¡ç±»å‹"""
    if task not in AVAILABLE_TASKS:
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid task '{task}'. Available: {AVAILABLE_TASKS}"
        )

"""
å¼‚æ­¥å‡½æ•°ï¼Œåœ¨åå°è¿è¡Œè®­ç»ƒä»»åŠ¡
å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€å’Œè¿›åº¦
å°†APIä»»åŠ¡ç±»å‹æ˜ å°„åˆ°å†…éƒ¨æ ¼å¼
"""

async def run_training_task(task_id: str, request: TrainingRequest):
    """è¿è¡Œè®­ç»ƒ+æ¨ç†+è¯„ä¼°ä»»åŠ¡ï¼Œå¹¶å®æ—¶æ•è·æ—¥å¿—"""
    orig_print = builtins.print
    try:
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        training_tasks[task_id].status = "running"
        training_tasks[task_id].progress = 0.0
        training_tasks[task_id].message = "Initializing training..."
        training_tasks[task_id].start_time = datetime.now()

        # 1. è§£æä»»åŠ¡ç±»å‹
        if request.task in ["nodeclass", "noderegress"]:
            task_level = "node"
            task_type  = "classification" if request.task=="nodeclass" else "regression"
        else:
            task_level = "edge"
            task_type  = "classification" if request.task=="edgeclass" else "regression"

        # 2. æ„é€  downstream_train æ‰€éœ€çš„ args
        args = argparse.Namespace(
            dataset=request.dataset_name,
            task_level=task_level,
            task=task_type,
            epochs=request.epochs,
            batch_size=request.batch_size,
            lr=request.lr,
            model=request.model,
            num_gnn_layers=request.num_gnn_layers,
            num_head_layers=request.num_head_layers,
            hid_dim=request.hid_dim,
            dropout=request.dropout,
            act_fn=request.act_fn,
            global_model_type=request.global_model_type,
            local_gnn_type=request.local_gnn_type,
            num_heads=request.num_heads,
            attn_dropout=request.attn_dropout,
            gpu=request.gpu,
            seed=request.seed,
            use_stats=request.use_stats,
            net_only=request.net_only,
            neg_edge_ratio=request.neg_edge_ratio,
            small_dataset_sample_rates=1.0,
            large_dataset_sample_rates=0.01,
            num_hops=4,
            num_neighbors=64,
            num_workers=0,
            sgrl=0,
            class_boundaries=[0.2,0.4,0.6,0.8] if task_type=="classification" else None,
            regress_loss="mse",
            class_loss="cross_entropy",
            num_classes=5 if task_type=="classification" else 1,
            src_dst_agg="concat",
            use_bn=0,
            residual=1,
            g_bn=1,
            g_drop=0.3,
            g_ffn=1,
            layer_norm=0,
            batch_norm=1,
            
            # ğŸ”§ å…³é”®æ·»åŠ 1ï¼šè®¾ç½®é»˜è®¤çš„åµŒå…¥å±‚å¤§å°
            node_type_vocab_size=10,  # é»˜è®¤å€¼ï¼Œå°†æ ¹æ®å®é™…æ•°æ®æ›´æ–°
            edge_type_vocab_size=10,  # é»˜è®¤å€¼ï¼Œå°†æ ¹æ®å®é™…æ•°æ®æ›´æ–°
        )

        # 3. è·å–æˆ–åˆ›å»º PyGraphDataset å°è£…å™¨
        training_tasks[task_id].progress = 10.0
        training_tasks[task_id].message = "Loading dataset..."
        
        dataset_key = f"{request.dataset_name}_{request.task}"
        if dataset_key not in dataset_cache:
            dataset_cache[dataset_key] = PyGraphDataset(
                name=request.dataset_name, task=request.task
            )
        wrapper = dataset_cache[dataset_key]
        
        if len(wrapper.dataset) == 0:
            raise ValueError("Dataset is empty")
            
        g = wrapper.dataset[0]
        print(f"Dataset info: {g.num_nodes} nodes, {g.num_edges} edges")
        
        # ğŸ”§ å…³é”®æ·»åŠ 2ï¼šæ ¹æ®å®é™…æ•°æ®æ›´æ–°åµŒå…¥å±‚å¤§å°
        if hasattr(g, 'node_type'):
            max_node_type = g.node_type.max().item()
            args.node_type_vocab_size = max_node_type + 1
            print(f"Updated node_type_vocab_size = {args.node_type_vocab_size} (max = {max_node_type})")
        
        if hasattr(g, 'edge_type'):
            max_edge_type = int(g.edge_type.max().item())
            args.edge_type_vocab_size = max_edge_type + 1
            print(f"Updated edge_type_vocab_size = {args.edge_type_vocab_size} (max = {max_edge_type})")

        # ğŸ”§ å…³é”®æ·»åŠ 3ï¼šä¿®å¤æ•°æ®ç±»å‹
        if hasattr(wrapper.dataset, 'data'):
            print("Fixing data types...")
            wrapper.dataset.data.node_type = wrapper.dataset.data.node_type.long()
            
            if hasattr(wrapper.dataset.data, 'edge_type'):
                print(f"Before: edge_type dtype = {wrapper.dataset.data.edge_type.dtype}")
                wrapper.dataset.data.edge_type = wrapper.dataset.data.edge_type.long()
                print(f"After: edge_type dtype = {wrapper.dataset.data.edge_type.dtype}")

        # 4. åˆ‡åˆ†ç´¢å¼• & æ„é€  DataLoader
        training_tasks[task_id].progress = 20.0
        training_tasks[task_id].message = "Creating data splits..."
        
        splits = wrapper.get_idx_split()
        
        # ğŸ”§ å…³é”®æ·»åŠ 4ï¼šä¿®å¤ç´¢å¼•èŒƒå›´é—®é¢˜
        def fix_indices_for_processed_graph(splits, graph, task):
            """ä¿®å¤ï¼šç¡®ä¿ç´¢å¼•åœ¨å¤„ç†åçš„å›¾èŒƒå›´å†…"""
            if task in ["nodeclass", "noderegress"]:
                max_valid_idx = graph.num_nodes - 1
            else:
                max_valid_idx = (graph.edge_label_index.shape[1] - 1 
                                if hasattr(graph, 'edge_label_index') 
                                else graph.num_edges - 1)
            
            print(f"Max valid index for {task}: {max_valid_idx}")
            
            fixed_splits = {}
            for split_name, indices in splits.items():
                original_indices = np.array(indices)
                valid_mask = (original_indices >= 0) & (original_indices <= max_valid_idx)
                valid_indices = original_indices[valid_mask]
                
                if len(valid_indices) != len(original_indices):
                    print(f"WARNING: {split_name} filtered {len(original_indices) - len(valid_indices)} invalid indices")
                
                if len(valid_indices) < len(original_indices) * 0.5:
                    print(f"WARNING: {split_name} lost >50% indices, resampling...")
                    num_needed = min(len(original_indices), max_valid_idx + 1)
                    if max_valid_idx + 1 >= num_needed:
                        valid_indices = np.random.choice(max_valid_idx + 1, size=num_needed, replace=False)
                    else:
                        valid_indices = np.arange(max_valid_idx + 1)
                
                fixed_splits[split_name] = valid_indices.tolist()
                print(f"  {split_name}: {len(fixed_splits[split_name])} indices")
            
            return fixed_splits

        splits = fix_indices_for_processed_graph(splits, g, request.task)
        
        # æŠŠ list/ndarray éƒ½ç»Ÿä¸€æˆ LongTensor
        train_idx = torch.tensor(splits["train"], dtype=torch.long)
        valid_idx = torch.tensor(splits["valid"], dtype=torch.long)
        test_idx = torch.tensor(splits["test"], dtype=torch.long)
        
        print(f"Final indices - train: {len(train_idx)}, valid: {len(valid_idx)}, test: {len(test_idx)}")
        
        # 5. åˆ›å»ºDataLoader
        training_tasks[task_id].progress = 30.0
        training_tasks[task_id].message = "Creating data loaders..."
        
        train_loader = wrapper.get_dataloader(train_idx)
        valid_loader = wrapper.get_dataloader(valid_idx)
        test_loader = wrapper.get_dataloader(test_idx)
        
        print("All data loaders created successfully")
        
        # 6. è®¾å¤‡è®¾ç½®
        device = torch.device(
            f"cuda:{request.gpu}" if torch.cuda.is_available() and request.gpu >= 0 else "cpu"
        )
        print(f"Using device: {device}")
        
        # ğŸ”§ å…³é”®æ·»åŠ 5ï¼šæµ‹è¯•ç¬¬ä¸€ä¸ªbatchç¡®ä¿æ²¡æœ‰é—®é¢˜
        print("Testing first batch before training...")
        try:
            for i, batch in enumerate(train_loader):
                batch = batch.to(device)
                print(f"First batch on {device}:")
                print(f"  node_type range: {batch.node_type.min()}-{batch.node_type.max()}")
                print(f"  edge_type range: {batch.edge_type.min()}-{batch.edge_type.max()}")
                print(f"  node_type dtype: {batch.node_type.dtype}")
                print(f"  edge_type dtype: {batch.edge_type.dtype}")
                
                # éªŒè¯èŒƒå›´ä¸ä¼šè¶Šç•Œ
                if batch.node_type.max() >= args.node_type_vocab_size:
                    raise ValueError(f"node_type {batch.node_type.max()} >= vocab_size {args.node_type_vocab_size}")
                if batch.edge_type.max() >= args.edge_type_vocab_size:
                    raise ValueError(f"edge_type {batch.edge_type.max()} >= vocab_size {args.edge_type_vocab_size}")
                
                if i == 0:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªbatch
                    break
            print("âœ… Batch test passed!")
        except Exception as batch_error:
            print(f"âŒ Batch test failed: {batch_error}")
            raise
        
        # ğŸ”§ å…³é”®æ·»åŠ 6ï¼šæœ€ç»ˆå‚æ•°ç¡®è®¤
        print(f"Final check before training:")
        print(f"  node_type_vocab_size: {args.node_type_vocab_size}")
        print(f"  edge_type_vocab_size: {args.edge_type_vocab_size}")
        
        # 7. æ•è·å†…éƒ¨ print è¾“å‡ºåˆ°ä»»åŠ¡æ¶ˆæ¯ä¸­
        def capture_print(*args, **kwargs):
            line = " ".join(str(a) for a in args)
            if task_id in training_tasks:
                training_tasks[task_id].message = line[:200]  # é™åˆ¶æ¶ˆæ¯é•¿åº¦
            orig_print(*args, **kwargs)
        builtins.print = capture_print

        # 8. çœŸæ­£è¿è¡Œè®­ç»ƒæµç¨‹
        training_tasks[task_id].progress = 50.0
        training_tasks[task_id].message = "Starting downstream_train..."
        
        downstream_train(args, wrapper.dataset, device, cl_embeds=None)

        # 9. æ¢å¤ print
        builtins.print = orig_print

        # 10. ç®€åŒ–ï¼šå…ˆè·³è¿‡è¯„ä¼°ï¼Œç›´æ¥æ ‡è®°å®Œæˆ
        training_tasks[task_id].status = "completed"
        training_tasks[task_id].progress = 100.0
        training_tasks[task_id].message = "Training completed successfully"
        training_tasks[task_id].end_time = datetime.now()

    except Exception as e:
        # æ¢å¤ print å¹¶æŠ¥é”™
        try:
            builtins.print = orig_print
        except:
            pass
        training_tasks[task_id].status = "failed"
        training_tasks[task_id].message = f"Training failed: {str(e)}"
        training_tasks[task_id].end_time = datetime.now()
        
        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print(f"Training task {task_id} failed: {e}")
        import traceback
        traceback.print_exc()


# ==================== APIç«¯ç‚¹ ====================

@app.get("/")
async def root():
    """APIæ ¹ç«¯ç‚¹"""
    return {
        "message": "RCG Circuit Graph Analysis API",
        "version": "1.0.0",
        "description": "API for training and evaluating graph neural networks on circuit data",
        "modules_available": MODULES_AVAILABLE,
        "available_datasets": AVAILABLE_DATASETS,
        "available_tasks": AVAILABLE_TASKS,
        "endpoints": {
            "create_dataset": "/api/dataset/create",
            "get_split": "/api/dataset/split", 
            "get_dataloader": "/api/dataset/dataloader",
            "train": "/api/train",
            "evaluate": "/api/evaluate",
            "visualize": "/api/visualize",
            "tasks": "/api/tasks"
        }
    }

@app.post("/api/dataset/create")
async def create_dataset(request: DatasetRequest):
    """
    åˆ›å»ºæ•°æ®é›† - å¯¹åº” dataset = PyGraphDataset(name="ssram", task="nodeclass")
    """
    try:
        validate_dataset(request.name)
        validate_task(request.task)
        
        # åˆ›å»ºPyGraphDatasetå®ä¾‹
        dataset = PyGraphDataset(name=request.name, task=request.task)
        
        # ç¼“å­˜æ•°æ®é›†
        dataset_key = f"{request.name}_{request.task}"
        dataset_cache[dataset_key] = dataset
        
        return {
            "dataset_id": dataset_key,
            "name": request.name,
            "task": request.task,
            "created_time": datetime.now().isoformat(),
            "message": f"Dataset {request.name} created successfully for task {request.task}"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create dataset: {str(e)}")

@app.post("/api/dataset/split")
async def get_dataset_split(request: DatasetRequest):
    """
    è·å–æ•°æ®åˆ†å‰² - å¯¹åº” split_idx = dataset.get_idx_split()
    """
    try:
        validate_dataset(request.name)
        validate_task(request.task)
        
        dataset_key = f"{request.name}_{request.task}"
        
        # å¦‚æœæ•°æ®é›†ä¸åœ¨ç¼“å­˜ä¸­ï¼Œå…ˆåˆ›å»º
        if dataset_key not in dataset_cache:
            dataset = PyGraphDataset(name=request.name, task=request.task)
            dataset_cache[dataset_key] = dataset
        else:
            dataset = dataset_cache[dataset_key]
        
        # è·å–åˆ†å‰²ç´¢å¼•

        split_idx = dataset.get_idx_split()
        
        return {
            "dataset_name": request.name,
            "task": request.task,
            "split_idx": {
                "train": split_idx["train"].tolist() if hasattr(split_idx["train"], 'tolist') else list(split_idx["train"]),
                "valid": split_idx["valid"].tolist() if hasattr(split_idx["valid"], 'tolist') else list(split_idx["valid"]),
                "test": split_idx["test"].tolist() if hasattr(split_idx["test"], 'tolist') else list(split_idx["test"])
            },
            "split_sizes": {
                "train": len(split_idx["train"]),
                "valid": len(split_idx["valid"]),
                "test": len(split_idx["test"])
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get dataset split: {str(e)}")

@app.post("/api/dataset/dataloader")
async def get_dataloader(request: DatasetRequest, split_type: str = "train"):
    """
    è·å–æ•°æ®åŠ è½½å™¨ - å¯¹åº” train_loader = dataset.get_dataloader(split_idx["train"])
    """
    try:
        validate_dataset(request.name)
        validate_task(request.task)
        
        if split_type not in ["train", "valid", "test"]:
            raise HTTPException(status_code=400, detail="split_type must be 'train', 'valid', or 'test'")
        
        dataset_key = f"{request.name}_{request.task}"
        
        # ç¡®ä¿æ•°æ®é›†å­˜åœ¨
        if dataset_key not in dataset_cache:
            dataset = PyGraphDataset(name=request.name, task=request.task)
            dataset_cache[dataset_key] = dataset
        else:
            dataset = dataset_cache[dataset_key]
        
        # è·å–åˆ†å‰²å’Œæ•°æ®åŠ è½½å™¨
        print(f"dataset:{dataset}")
        split_idx = dataset.get_idx_split()
        dataloader_info = dataset.get_dataloader(split_idx[split_type])
        
        return {
            "dataset_name": request.name,
            "task": request.task,
            "split_type": split_type,
            "dataloader_created": True,
            "split_size": len(split_idx[split_type]),
            "message": f"DataLoader created for {split_type} split"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create dataloader: {str(e)}")

@app.post("/api/train")
async def start_training(request: TrainingRequest, background_tasks: BackgroundTasks):
    """
    å¯åŠ¨è®­ç»ƒä»»åŠ¡ - ä½¿ç”¨ä½ çš„å®Œæ•´è®­ç»ƒæµç¨‹
    """
    try:
        validate_dataset(request.dataset_name)
        validate_task(request.task)
        
        # åˆ›å»ºä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        training_tasks[task_id] = TaskStatus(
            task_id=task_id,
            status="pending",
            progress=0.0,
            message="Training task created",
            start_time=datetime.now()
        )
        
        # å¯åŠ¨åå°è®­ç»ƒ
        background_tasks.add_task(run_training_task, task_id, request)
        
        return {
            "task_id": task_id,
            "dataset": request.dataset_name,
            "task": request.task,
            "model": request.model,
            "status": "pending",
            "message": f"Training started for {request.dataset_name} with {request.model} model"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start training: {str(e)}")

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    """è·å–è®­ç»ƒä»»åŠ¡çŠ¶æ€"""
    if task_id not in training_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return training_tasks[task_id]

@app.get("/api/tasks")
async def list_tasks(status: Optional[str] = None, limit: int = 20):
    """åˆ—å‡ºæ‰€æœ‰è®­ç»ƒä»»åŠ¡"""
    tasks = list(training_tasks.values())
    
    if status:
        tasks = [task for task in tasks if task.status == status]
    
    tasks.sort(key=lambda x: x.start_time, reverse=True)
    return {"tasks": tasks[:limit], "total": len(tasks)}

@app.post("/api/evaluate")
async def evaluate_model(request: EvaluationRequest):
    """
    è¯„ä¼°æ¨¡å‹ - å¯¹åº” evaluator = Evaluator(name="ssram", task="nodeclass")
                  result_dict = evaluator.eval(input_dict)
    """
    try:
        validate_dataset(request.dataset_name)
        validate_task(request.task)
        
        if len(request.y_true) != len(request.y_pred):
            raise HTTPException(status_code=400, detail="y_true and y_pred must have the same length")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = Evaluator(name=request.dataset_name, task=request.task)
        
        # å‡†å¤‡è¾“å…¥å­—å…¸
        input_dict = {
            "y_true": request.y_true,
            "y_pred": request.y_pred
        }
        
        # æ‰§è¡Œè¯„ä¼°
        result_dict = evaluator.eval(input_dict)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_id = str(uuid.uuid4())
        evaluation_results[eval_id] = {
            "eval_id": eval_id,
            "dataset": request.dataset_name,
            "task": request.task,
            "results": result_dict,
            "evaluation_time": datetime.now(),
            "sample_count": len(request.y_true)
        }
        
        return {
            "eval_id": eval_id,
            "dataset": request.dataset_name,
            "task": request.task,
            "results": result_dict,
            "sample_count": len(request.y_true),
            "evaluation_time": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Evaluation failed: {str(e)}")

@app.post("/api/visualize")
@app.post("/api/visualize")
async def create_visualization(request: VisualizationRequest):
    """
    åˆ›å»ºå¯è§†åŒ– - ä½¿ç”¨ä½ çš„plotæ¨¡å—
    """
    try:
        validate_dataset(request.dataset_name)

        if MODULES_AVAILABLE:
            # ä½¿ç”¨çœŸå®çš„å¯è§†åŒ–æ¨¡å—
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt

            os.makedirs("./imgs", exist_ok=True)

            dataset = SealSramDataset(
                name=request.dataset_name,
                root="./datasets/",
                task_level=request.task_level
            )
            if len(dataset) > 0:
                # åŠ è½½åŸå§‹å›¾ï¼ˆåŒ…å« tar_node_y / tar_edge_yï¼‰
                raw_path = os.path.join(dataset.folder, "raw", f"{dataset.names[0]}.pt")
                g = dataset.sram_graph_load(dataset.names[0], raw_path)

                # å°†è¾¹ç•Œä» list è½¬ Tensor
                device = g.tar_edge_y.device if request.task_level == "edge" else g.tar_node_y.device
                boundaries = torch.tensor(
                    request.class_boundaries,
                    dtype=torch.float32,
                    device=device
                )

                # è°ƒç”¨å¯è§†åŒ–å¹¶è®°å½•ä¿å­˜çš„åŸå§‹æ–‡ä»¶å
                if request.task_level == "node":
                    visualize_node_label_distribution(g, request.dataset_name, boundaries)
                    saved_name = f"node_label_dist_{request.dataset_name}.png"
                else:
                    visualize_edge_label_distribution(g, request.dataset_name, boundaries)
                    saved_name = f"edge_label_dist_{request.dataset_name}.png"

                # é‡å‘½åä¸º API è¿”å›çš„ desired æ ¼å¼
                real_path = os.path.join("imgs", saved_name)
                desired_name = f"{request.task_level}_dist_{request.dataset_name}.png"
                desired_path = os.path.join("imgs", desired_name)
                if saved_name != desired_name and os.path.exists(real_path):
                    os.replace(real_path, desired_path)

                output_path = f"./imgs/{desired_name}"
        else:
            # æ¨¡æ‹Ÿå¯è§†åŒ–
            output_path = f"./imgs/{request.task_level}_dist_{request.dataset_name}.png"

        return {
            "dataset": request.dataset_name,
            "task_level": request.task_level,
            "visualization_path": output_path,
            "class_boundaries": request.class_boundaries,
            "created_time": datetime.now().isoformat()
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Visualization failed: {str(e)}")



@app.get("/api/datasets")
async def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†"""
    dataset_info = []
    
    for dataset_name in AVAILABLE_DATASETS:
        dataset_path = f"./datasets/{dataset_name}.pt"
        exists = os.path.exists(dataset_path)
        
        info = {
            "name": dataset_name,
            "path": dataset_path,
            "exists": exists,
            "size_bytes": os.path.getsize(dataset_path) if exists else None,
            "supported_tasks": AVAILABLE_TASKS
        }
        dataset_info.append(info)
    
    return {
        "available_datasets": dataset_info,
        "total_count": len(AVAILABLE_DATASETS),
        "modules_available": MODULES_AVAILABLE
    }

@app.get("/api/system/status")
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    return {
        "modules_available": MODULES_AVAILABLE,
        "available_datasets": AVAILABLE_DATASETS,
        "available_tasks": AVAILABLE_TASKS,
        "active_training_tasks": len([t for t in training_tasks.values() if t.status == "running"]),
        "total_training_tasks": len(training_tasks),
        "total_evaluations": len(evaluation_results),
        "cached_datasets": len(dataset_cache),
        "gpu_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
    }

# ==================== å¯åŠ¨é…ç½® ====================

if __name__ == "__main__":  
    # ä»…åœ¨ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶æ‰ä¼šæ‰§è¡Œä»¥ä¸‹ä»£ç å—
    # è¿™ä¸ªæ¡ä»¶ç¡®ä¿å¦‚æœæ­¤æ–‡ä»¶ä½œä¸ºæ¨¡å—å¯¼å…¥æ—¶ï¼Œä¸ä¼šæ‰§è¡Œä»¥ä¸‹ä»£ç 

    # åˆ›å»ºå­˜æ”¾æ•°æ®é›†çš„ç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œåˆ™ä¸ä¼šæŠ›å‡ºé”™è¯¯
    os.makedirs("./datasets", exist_ok=True)

    # åˆ›å»ºå­˜æ”¾ç”Ÿæˆå›¾åƒçš„ç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œåˆ™ä¸ä¼šæŠ›å‡ºé”™è¯¯
    os.makedirs("./imgs", exist_ok=True)

    # åˆ›å»ºå­˜æ”¾æ—¥å¿—çš„ç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œåˆ™ä¸ä¼šæŠ›å‡ºé”™è¯¯
    os.makedirs("./logs", exist_ok=True)

    # åœ¨æ§åˆ¶å°æ‰“å°å¯åŠ¨ä¿¡æ¯ï¼Œè¡¨ç¤º API æ­£åœ¨å¯åŠ¨
    print("ğŸš€ Starting RCG Circuit Graph Analysis API...")

    # æ‰“å°æ˜¯å¦èƒ½å¤ŸæˆåŠŸåŠ è½½æ¨¡å—
    # å¦‚æœ MODULES_AVAILABLE ä¸º Trueï¼Œè¯´æ˜èƒ½å¤ŸåŠ è½½çœŸå®æ¨¡å—ï¼Œå¦åˆ™ä¼šè¿›å…¥æ¨¡æ‹Ÿæ¨¡å¼
    print(f"ğŸ“¦ Modules available: {MODULES_AVAILABLE}")

    # æ‰“å°å¯ç”¨çš„æ•°æ®é›†åˆ—è¡¨ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£å¯ä»¥ä½¿ç”¨çš„è®­ç»ƒæ•°æ®
    print(f"ğŸ”§ Available datasets: {AVAILABLE_DATASETS}")

    # æ‰“å°å¯ç”¨çš„ä»»åŠ¡ç±»å‹ï¼ˆå¦‚èŠ‚ç‚¹åˆ†ç±»ã€å›å½’ç­‰ï¼‰ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ”¯æŒçš„ä»»åŠ¡ç±»å‹
    print(f"ğŸ“Š Available tasks: {AVAILABLE_TASKS}")


    # å¯åŠ¨ Uvicorn æœåŠ¡ï¼Œè¿™å°†ä¼šå¯åŠ¨ FastAPI åº”ç”¨
    uvicorn.run(
        "api:app",        # ä»¥å­—ç¬¦ä¸²çš„å½¢å¼æŒ‡å®šæ¨¡å—å’Œåº”ç”¨å®ä¾‹ï¼Œæ¨¡å—åä¸º "api"ï¼ˆä¸å¸¦ .pyï¼‰ï¼Œapp æ˜¯ FastAPI å®ä¾‹
        host="0.0.0.0",   # æœåŠ¡å°†ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£ï¼Œå…è®¸å¤–éƒ¨è®¿é—®
        port=8000,        # è®¾ç½®æœåŠ¡å™¨ç«¯å£ä¸º 8000
        reload=True,      # å¯ç”¨ä»£ç çƒ­é‡è½½ï¼Œæ–¹ä¾¿å¼€å‘è°ƒè¯•ï¼Œæºä»£ç å˜åŠ¨æ—¶è‡ªåŠ¨é‡å¯åº”ç”¨
    )


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
"""
def create_dataset(name, task):
    \"\"\"åˆ›å»ºæ•°æ®é›† - å¯¹åº” PyGraphDataset(name="ssram", task="nodeclass")\"\"\"
    response = requests.post(f"{BASE_URL}/api/dataset/create", 
                           json={"name": name, "task": task})
    return response.json()

def get_dataset_split(name, task):
    \"\"\"è·å–æ•°æ®åˆ†å‰² - å¯¹åº” split_idx = dataset.get_idx_split()\"\"\"
    response = requests.post(f"{BASE_URL}/api/dataset/split",
                           json={"name": name, "task": task})
    return response.json()

def get_dataloader(name, task, split_type="train"):
    \"\"\"è·å–æ•°æ®åŠ è½½å™¨ - å¯¹åº” train_loader = dataset.get_dataloader(split_idx["train"])\"\"\"
    response = requests.post(f"{BASE_URL}/api/dataset/dataloader?split_type={split_type}",
                           json={"name": name, "task": task})
    return response.json()

def start_training(config):
    \"\"\"å¯åŠ¨è®­ç»ƒ - ä½¿ç”¨ä½ çš„å®Œæ•´è®­ç»ƒæµç¨‹\"\"\"
    response = requests.post(f"{BASE_URL}/api/train", json=config)
    return response.json()

def evaluate_model(dataset_name, task, y_true, y_pred):
    \"\"\"è¯„ä¼°æ¨¡å‹ - å¯¹åº” evaluator = Evaluator(...) å’Œ evaluator.eval(input_dict)\"\"\"
    response = requests.post(f"{BASE_URL}/api/evaluate", 
                           json={
                               "dataset_name": dataset_name,
                               "task": task,
                               "y_true": y_true,
                               "y_pred": y_pred
                           })
    return response.json()

def create_visualization(dataset_name, task_level="node"):
    \"\"\"åˆ›å»ºå¯è§†åŒ– - ä½¿ç”¨ä½ çš„plotæ¨¡å—\"\"\"
    response = requests.post(f"{BASE_URL}/api/visualize",
                           json={
                               "dataset_name": dataset_name,
                               "task_level": task_level,
                               "class_boundaries": [0.2, 0.4, 0.6, 0.8]
                           })
    return response.json()

def get_task_status(task_id):
    \"\"\"æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€\"\"\"
    response = requests.get(f"{BASE_URL}/api/tasks/{task_id}")
    return response.json()


è¿™ä¸ªAPIå®Œå…¨æ¨¡æ‹Ÿäº†ä½ çš„ä»£ç ç»“æ„å’Œä½¿ç”¨æ–¹å¼ï¼š

âœ… **å¯¹åº”å…³ç³»**:
- `PyGraphDataset(name="ssram", task="nodeclass")` â¡ï¸ `POST /api/dataset/create`
- `dataset.get_idx_split()` â¡ï¸ `POST /api/dataset/split`
- `dataset.get_dataloader(split_idx["train"])` â¡ï¸ `POST /api/dataset/dataloader`
- `downstream_train(args, dataset, device)` â¡ï¸ `POST /api/train`
- `Evaluator(name="ssram", task="nodeclass")` â¡ï¸ `POST /api/evaluate`
- `evaluator.eval(input_dict)` â¡ï¸ è¯„ä¼°é€»è¾‘
- `visualize_node_label_distribution()` â¡ï¸ `POST /api/visualize`

âœ… **åŠŸèƒ½ç‰¹ç‚¹**:
- å®Œå…¨ä½¿ç”¨ä½ çš„8æ®µä»£ç æ¨¡å—
- æ”¯æŒä½ çš„7ä¸ªæ•°æ®é›†
- æ”¯æŒæ‰€æœ‰ä½ çš„æ¨¡å‹ç±»å‹å’Œå‚æ•°
- å®æ—¶è®­ç»ƒç›‘æ§å’Œæ—¥å¿—
- å®Œæ•´çš„è¯„ä¼°æµç¨‹
- æ•°æ®å¯è§†åŒ–åŠŸèƒ½

ç°åœ¨ä½ å¯ä»¥é€šè¿‡APIçš„æ–¹å¼ä½¿ç”¨ä½ çš„æ‰€æœ‰ä»£ç ï¼Œå°±åƒç›´æ¥è°ƒç”¨Pythonå‡½æ•°ä¸€æ ·æ–¹ä¾¿ï¼
"""
